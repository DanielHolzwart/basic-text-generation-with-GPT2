{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP4xRgjyjQBg6T6ExQlL0VQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanielHolzwart/basic-text-generation-with-GPT2/blob/main/Text_Generation_with_GPT2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading a pretrained text generation model like GPT2 we can quickly generate text via the generate function. Moreover, this function allows us to specify on top k & top p sampling, greedy search,.... Here we want to implement our own custom functions for a couple of such options."
      ],
      "metadata": {
        "id": "m6Lp-tJc3F46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our test model is going to be GPT2. Start loading it from GPT2 hugging face."
      ],
      "metadata": {
        "id": "D-tX8cKb3vXd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOJoHVsWemkG",
        "outputId": "794904d5-b6a8-4d69-f466-c53257e114e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "lYys-hwNgHfE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49b3a22b-0c39-4bf9-eca6-0a2c0068990f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "My girlfriend's dog is called Joey and he will provide our example text 'The dog Joey is jumping over the'"
      ],
      "metadata": {
        "id": "2WRLkzO339KI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'The dog Joey is jumping over the'"
      ],
      "metadata": {
        "id": "YhnZkkngew20"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, we are going to play around with the tokenization of this text."
      ],
      "metadata": {
        "id": "ox0IOpuP4R1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_input_ids = tokenizer(text).input_ids #extract input_ids from tokenized text\n",
        "print(text_input_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yl7JVcmS4kRv",
        "outputId": "6c1fd5ea-727c-4008-d37f-b9aaef7fabd6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[464, 3290, 26154, 318, 14284, 625, 262]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_tokens = tokenizer.convert_ids_to_tokens(text_input_ids) #this shows how GPT2 tokenizes words. Note the dot above the words which correspond to spacebars between words\n",
        "print(text_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IA02_x0M46yr",
        "outputId": "f3a19a9c-906b-416f-e781-99694289307b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'Ġdog', 'ĠJoey', 'Ġis', 'Ġjumping', 'Ġover', 'Ġthe']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_id_to_text = tokenizer.decode(text_input_ids) #convert input_ids back to the text\n",
        "print(text_id_to_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQl3lyoz5UkZ",
        "outputId": "e53f548b-3546-473e-8012-71bd2e135a5f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dog Joey is jumping over the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_tokens_to_text = tokenizer.convert_tokens_to_string(text_tokens) #in the same way we can convert tokens back to the text\n",
        "print(text_tokens_to_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDIs0IEx6Q8b",
        "outputId": "6941c35b-c132-4a69-a325-ededb99a71ec"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dog Joey is jumping over the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------------------------------------\n",
        "# **1.1 Construct a a simple generate function**\n",
        "\n",
        "Our aim now is to replicate the following sentence(s)"
      ],
      "metadata": {
        "id": "swx5JWMz8LZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token_id = tokenizer.eos_token_id # set padding token id to end of sentence token id for open-end generation\n",
        "text_pt = tokenizer(text, return_tensors = 'pt').to(device)\n",
        "output = model.generate(**text_pt,max_new_tokens = 50, pad_token_id = tokenizer.eos_token_id )\n",
        "print(tokenizer.decode(output.squeeze(0)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyHyFjwC_UNS",
        "outputId": "5ef584b1-7453-4d32-9c2c-0327584d8a11"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dog Joey is jumping over the fence and into the water. He is about to jump over the fence when he is hit by a car. Joey is taken to the hospital where he is treated for his injuries.\n",
            "\n",
            "The dog Joey is jumping over the fence and into the water\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the omdel already repeats the sentences as it lacks creativity since it is only doing greedy search. We could change that by using a few options (like beam search or sampling) which we will talk about later."
      ],
      "metadata": {
        "id": "Tf94f3NIA61o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_words(max_new_tokens, text=text):\n",
        "    input = text\n",
        "    for _ in range(max_new_tokens + 1):\n",
        "        output = model(**tokenizer(input, return_tensors = 'pt').to(device)) #output shape is (batch_size, tokens, embedding_dim)\n",
        "        logits = output.logits[0,-1,:] #return logits for the last word the text (so for the initial text it is 'the')\n",
        "        max_prob = torch.argmax(logits)\n",
        "        input = input + tokenizer.decode(max_prob)\n",
        "    return input"
      ],
      "metadata": {
        "id": "IgEe4Qbi8LAM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_words(50))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdLuq4UKC1Pd",
        "outputId": "4d3c951d-7784-4eaa-8871-4e5cec386f32"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dog Joey is jumping over the fence and into the water. He is about to jump over the fence when he is hit by a car. Joey is taken to the hospital where he is treated for his injuries.\n",
            "\n",
            "\n",
            "The dog Joey is jumping over the fence and into the water\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also store the five words with the highest probablity in a dataframe. For this, we are going to slightly adjust the function above. This will also be useful for sampling methods."
      ],
      "metadata": {
        "id": "ZK8SxNX7KfUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "QcwPBZsYoSdl"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(**tokenizer(text, return_tensors = 'pt').to(device)) #output shape is (batch_size, tokens, embedding_dim)\n",
        "logits = output.logits[0,-1,:] #return logits for the last word the text (e.g. for the initial text it is 'the')\n",
        "\n",
        "probs_sorted = torch.sort(logits, dim = - 1, descending = True)\n",
        "probs_sorted_indices = probs_sorted.indices"
      ],
      "metadata": {
        "id": "snl7aijerDtx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(probs_sorted_indices[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "j2oezmS2rLYv",
        "outputId": "384d1070-75fb-4a79-a05c-241f21559426"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' fence'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def probability_df(max_new_tokens, text=text, top_probs = 5):\n",
        "    dt_frame = {}\n",
        "    input = text\n",
        "    for i in range(max_new_tokens + 1):\n",
        "        output = model(**tokenizer(input, return_tensors = 'pt').to(device)) #output shape is (batch_size, tokens, embedding_dim)\n",
        "        logits = output.logits[0,-1,:] #return logits for the last word the text (e.g. for the initial text it is 'the')\n",
        "\n",
        "        probs_sorted = torch.sort(logits, dim = - 1, descending = True)\n",
        "        probs_sorted_indices = probs_sorted.indices\n",
        "        probs_sorted_values = probs_sorted.values\n",
        "        probs_sorted_values = torch.softmax(probs_sorted_values, dim = -1)\n",
        "\n",
        "        dt_frame[f'Text step {i+1}'] = [input]\n",
        "\n",
        "        for j in range(top_probs):\n",
        "            word = tokenizer.decode(probs_sorted_indices[j])\n",
        "            dt_frame[f'Text step {i+1}'].append(f'{word} ({probs_sorted_values[j]:.2%})')\n",
        "\n",
        "        input = input + tokenizer.decode(probs_sorted_indices[0])\n",
        "\n",
        "    return pd.DataFrame(dt_frame).T"
      ],
      "metadata": {
        "id": "R0qaAaNTKqBN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function generate_words_with_df a single input generates 5 additional words to the initial sentence and output a dataframe with the 5 highest word probabilities after every step. The percentage output is the probability of that token being taken if we would have randomly sampled."
      ],
      "metadata": {
        "id": "4yWUUikpolv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "probability_df(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "HUFDrl8ZSFdS",
        "outputId": "d80e1d4f-f6bb-4b29-bcd9-8750779496cb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                             0  \\\n",
              "Text step 1                   The dog Joey is jumping over the   \n",
              "Text step 2             The dog Joey is jumping over the fence   \n",
              "Text step 3         The dog Joey is jumping over the fence and   \n",
              "Text step 4    The dog Joey is jumping over the fence and into   \n",
              "Text step 5  The dog Joey is jumping over the fence and int...   \n",
              "\n",
              "                           1                 2               3  \\\n",
              "Text step 1   fence (23.04%)   railing (8.62%)    edge (3.04%)   \n",
              "Text step 2     and (13.09%)        , (11.11%)      . (10.19%)   \n",
              "Text step 3     into (5.23%)        is (4.44%)    onto (3.25%)   \n",
              "Text step 4     the (62.16%)        a (16.68%)      my (2.36%)   \n",
              "Text step 5   water (13.76%)    bushes (4.06%)   river (3.33%)   \n",
              "\n",
              "                            4                5  \n",
              "Text step 1      wall (2.96%)   bridge (2.17%)  \n",
              "Text step 2        to (8.67%)       is (4.08%)  \n",
              "Text step 3   running (3.18%)      the (2.04%)  \n",
              "Text step 4       his (2.01%)       an (1.45%)  \n",
              "Text step 5      yard (2.22%)   street (1.78%)  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a346bf24-dafc-438b-89e1-fa070d69ba48\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Text step 1</th>\n",
              "      <td>The dog Joey is jumping over the</td>\n",
              "      <td>fence (23.04%)</td>\n",
              "      <td>railing (8.62%)</td>\n",
              "      <td>edge (3.04%)</td>\n",
              "      <td>wall (2.96%)</td>\n",
              "      <td>bridge (2.17%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Text step 2</th>\n",
              "      <td>The dog Joey is jumping over the fence</td>\n",
              "      <td>and (13.09%)</td>\n",
              "      <td>, (11.11%)</td>\n",
              "      <td>. (10.19%)</td>\n",
              "      <td>to (8.67%)</td>\n",
              "      <td>is (4.08%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Text step 3</th>\n",
              "      <td>The dog Joey is jumping over the fence and</td>\n",
              "      <td>into (5.23%)</td>\n",
              "      <td>is (4.44%)</td>\n",
              "      <td>onto (3.25%)</td>\n",
              "      <td>running (3.18%)</td>\n",
              "      <td>the (2.04%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Text step 4</th>\n",
              "      <td>The dog Joey is jumping over the fence and into</td>\n",
              "      <td>the (62.16%)</td>\n",
              "      <td>a (16.68%)</td>\n",
              "      <td>my (2.36%)</td>\n",
              "      <td>his (2.01%)</td>\n",
              "      <td>an (1.45%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Text step 5</th>\n",
              "      <td>The dog Joey is jumping over the fence and int...</td>\n",
              "      <td>water (13.76%)</td>\n",
              "      <td>bushes (4.06%)</td>\n",
              "      <td>river (3.33%)</td>\n",
              "      <td>yard (2.22%)</td>\n",
              "      <td>street (1.78%)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a346bf24-dafc-438b-89e1-fa070d69ba48')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a346bf24-dafc-438b-89e1-fa070d69ba48 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a346bf24-dafc-438b-89e1-fa070d69ba48');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f842db9e-4aee-4ee0-a84b-731a50a59fed\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f842db9e-4aee-4ee0-a84b-731a50a59fed')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f842db9e-4aee-4ee0-a84b-731a50a59fed button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"probability_df(4)\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"The dog Joey is jumping over the fence\",\n          \"The dog Joey is jumping over the fence and into the\",\n          \"The dog Joey is jumping over the fence and\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 1,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \" and (13.09%)\",\n          \" water (13.76%)\",\n          \" into (5.23%)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 2,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \", (11.11%)\",\n          \" bushes (4.06%)\",\n          \" is (4.44%)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 3,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \". (10.19%)\",\n          \" river (3.33%)\",\n          \" onto (3.25%)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 4,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \" to (8.67%)\",\n          \" yard (2.22%)\",\n          \" running (3.18%)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 5,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \" is (4.08%)\",\n          \" street (1.78%)\",\n          \" the (2.04%)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.2 Create a generate function with top_k sampling**\n",
        "\n",
        "Next we are going to tackle the topic of top-k sampling. In all the words created above we simple used greedy search which means that we added the word with the highest output probability. In top-k sampling we randomly sample from, let's say 50, words and the output is much cleaner, non-repetetive and something more similar to human generated text."
      ],
      "metadata": {
        "id": "aeLOd3wSth14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, we first used the convient generate functon to which is giving us the output we want to create."
      ],
      "metadata": {
        "id": "VQkXhGXVuCsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "output = model.generate(**text_pt,max_new_tokens = 20, pad_token_id = tokenizer.eos_token_id, top_k =3, do_sample = True)\n",
        "print(tokenizer.decode(output.squeeze(0)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGxNFvAVueEm",
        "outputId": "eaf54c78-8125-444c-db93-81afb2c6fac5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dog Joey is jumping over the edge of the wall and is trying to get to the bottom of it, but it's too late\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Via the categorical class from torch.distributions we can sample data based on the relative probabilities."
      ],
      "metadata": {
        "id": "jHiAapjSxq_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributions.categorical import Categorical\n",
        "logits = torch.tensor([[.1, .1, .8]])\n",
        "m = Categorical(logits=logits)\n",
        "samples = m.sample((10,))\n",
        "print(samples.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYwIJI9qxl3C",
        "outputId": "48d6f8ed-bcd6-4000-ed59-12a0f45e0eb4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [2]\n",
            " [1]\n",
            " [2]\n",
            " [2]\n",
            " [0]\n",
            " [0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_topk(max_new_tokens, top_k, text=text):\n",
        "    torch.manual_seed(42)\n",
        "    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "    for _ in range(max_new_tokens + 1):\n",
        "        output = model(input_ids) #output shape is (batch_size, tokens, embedding_dim)\n",
        "        logits = output.logits[0,-1,:] #return logits for the last word the text (e.g. for the initial text it is 'the')\n",
        "\n",
        "        top_k_logits, top_k_indices = torch.topk(logits, k=top_k) # we can also use the topk function instead of ordering and taking the top k values\n",
        "        top_k_probabilities = torch.softmax(top_k_logits, dim = -1)\n",
        "\n",
        "        m = Categorical(probs=top_k_probabilities).sample()\n",
        "        next_token_id = top_k_indices[m].unsqueeze(0)\n",
        "\n",
        "        input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0)], dim=-1)\n",
        "\n",
        "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "    return generated_text"
      ],
      "metadata": {
        "id": "1_ZkVd_0uric"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text_topk(20,3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "yHWE04pHy1Eq",
        "outputId": "2f964c93-388a-4310-89ea-971d8bac05e8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The dog Joey is jumping over the fence and into the bushes.\\n\\n\"I\\'m not sure how he got there, but it looks'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the output of both sentences are not the same. My first assumption is that sampling in the generate function and in the custom function via Categorical is different. Possibly the top_k values are the same input_ids, but the probabilities are just different. Let us store additional data in a\n",
        "dataframe similar to the probability_df function above."
      ],
      "metadata": {
        "id": "w4xj-rgzmRuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def df_topk(max_new_tokens, top_k, text=text):\n",
        "    dt_frame = {}\n",
        "    torch.manual_seed(42)\n",
        "    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
        "    for i in range(max_new_tokens + 1):\n",
        "        output = model(input_ids) #output shape is (batch_size, tokens, embedding_dim)\n",
        "        logits = output.logits[0,-1,:] #return logits for the last word the text (e.g. for the initial text it is 'the')\n",
        "\n",
        "        top_k_logits, top_k_indices = torch.topk(logits, k=top_k) # we can also use the topk function instead of ordering and taking the top k values\n",
        "        top_k_probabilities = torch.softmax(top_k_logits, dim = -1)\n",
        "\n",
        "        dt_frame[f'Text step {i+1}'] = [tokenizer.decode(input_ids[0], skip_special_tokens=True)]\n",
        "\n",
        "        for j in range(top_k):\n",
        "            word = tokenizer.decode(top_k_indices[j])\n",
        "            dt_frame[f'Text step {i+1}'].append(f'{word} ({top_k_probabilities[j]:.2%})')\n",
        "\n",
        "        m = Categorical(logits=top_k_logits).sample()\n",
        "        next_token_id = top_k_indices[m].unsqueeze(0)\n",
        "\n",
        "        input_ids = torch.cat([input_ids, next_token_id.unsqueeze(0)], dim=-1)\n",
        "\n",
        "    return pd.DataFrame(dt_frame).T"
      ],
      "metadata": {
        "id": "bVbAWOJJmQ-4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_topk(5,3)[0].iloc[5]) #output of the full sentence\n",
        "df_topk(5,3) #output probability dataframe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "5MkEG2dOotkr",
        "outputId": "649f6a9d-08cb-4ebc-93e6-bf105c9425f8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dog Joey is jumping over the fence and into the bushes\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                             0  \\\n",
              "Text step 1                   The dog Joey is jumping over the   \n",
              "Text step 2             The dog Joey is jumping over the fence   \n",
              "Text step 3         The dog Joey is jumping over the fence and   \n",
              "Text step 4    The dog Joey is jumping over the fence and into   \n",
              "Text step 5  The dog Joey is jumping over the fence and int...   \n",
              "Text step 6  The dog Joey is jumping over the fence and int...   \n",
              "\n",
              "                           1                  2                3  \n",
              "Text step 1   fence (66.40%)   railing (24.84%)     edge (8.76%)  \n",
              "Text step 2     and (38.06%)         , (32.31%)       . (29.64%)  \n",
              "Text step 3    into (40.50%)        is (34.36%)    onto (25.14%)  \n",
              "Text step 4     the (76.55%)         a (20.54%)       my (2.91%)  \n",
              "Text step 5   water (65.04%)    bushes (19.21%)   river (15.76%)  \n",
              "Text step 6       . (66.35%)         , (27.01%)       to (6.64%)  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1010e53d-3218-4680-ae29-facf86175df1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Text step 1</th>\n",
              "      <td>The dog Joey is jumping over the</td>\n",
              "      <td>fence (66.40%)</td>\n",
              "      <td>railing (24.84%)</td>\n",
              "      <td>edge (8.76%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Text step 2</th>\n",
              "      <td>The dog Joey is jumping over the fence</td>\n",
              "      <td>and (38.06%)</td>\n",
              "      <td>, (32.31%)</td>\n",
              "      <td>. (29.64%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Text step 3</th>\n",
              "      <td>The dog Joey is jumping over the fence and</td>\n",
              "      <td>into (40.50%)</td>\n",
              "      <td>is (34.36%)</td>\n",
              "      <td>onto (25.14%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Text step 4</th>\n",
              "      <td>The dog Joey is jumping over the fence and into</td>\n",
              "      <td>the (76.55%)</td>\n",
              "      <td>a (20.54%)</td>\n",
              "      <td>my (2.91%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Text step 5</th>\n",
              "      <td>The dog Joey is jumping over the fence and int...</td>\n",
              "      <td>water (65.04%)</td>\n",
              "      <td>bushes (19.21%)</td>\n",
              "      <td>river (15.76%)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Text step 6</th>\n",
              "      <td>The dog Joey is jumping over the fence and int...</td>\n",
              "      <td>. (66.35%)</td>\n",
              "      <td>, (27.01%)</td>\n",
              "      <td>to (6.64%)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1010e53d-3218-4680-ae29-facf86175df1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1010e53d-3218-4680-ae29-facf86175df1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1010e53d-3218-4680-ae29-facf86175df1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4e7c4762-c6d0-4a1e-beaf-28db89d3ebe4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4e7c4762-c6d0-4a1e-beaf-28db89d3ebe4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4e7c4762-c6d0-4a1e-beaf-28db89d3ebe4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df_topk(5,3) #output probability dataframe\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"The dog Joey is jumping over the\",\n          \"The dog Joey is jumping over the fence\",\n          \"The dog Joey is jumping over the fence and into the bushes\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 1,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \" fence (66.40%)\",\n          \" and (38.06%)\",\n          \". (66.35%)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 2,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \" railing (24.84%)\",\n          \", (32.31%)\",\n          \", (27.01%)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 3,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \" edge (8.76%)\",\n          \". (29.64%)\",\n          \" to (6.64%)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the datatable we can nicely see that in the 5th row our algorithm did not take the highest probability but rather the 2nd highest word 'bushes'. In particular, our algorithm takes the word 'fence' in the first step, while it takes the word 'edge' in the generate function."
      ],
      "metadata": {
        "id": "cRb7ghc0q9s8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "print(tokenizer.decode(model.generate(**text_pt,max_new_tokens = 1, top_k=3, do_sample = True, pad_token_id = tokenizer.eos_token_id)[0]))\n",
        "print(tokenizer.decode(model.generate(**text_pt,max_new_tokens = 1, do_sample = True, pad_token_id = tokenizer.eos_token_id)[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9ZoPHqw8JT3",
        "outputId": "84a80acc-09b9-4948-d3f5-019a01caaa53"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dog Joey is jumping over the edge\n",
            "The dog Joey is jumping over the river\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will leave it like this for the moment, but keep it in our mind for the following tasks."
      ],
      "metadata": {
        "id": "Bl6h3O1A8SsC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.3 Create a generate function with top_p sampling**"
      ],
      "metadata": {
        "id": "JdAYj0228bYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Top_p sampling, also known as nucleus sampling, is another sampling approach to generate better texts. Here we don't specify on the number of tokens to sample from, but rather the cummulative sum o the tokens. For example, if top_p = 0.95 then we would sample from all tokens, starting with the highest probability, such that their cummulative sum is less or equal 95%. Hence, at different time steps we pool of tokens to chose from is not the same, only the upper bound remains constant."
      ],
      "metadata": {
        "id": "oNFkr-pm9An-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "tokenizer.batch_decode(model.generate(**text_pt,max_new_tokens = 20, pad_token_id = tokenizer.eos_token_id, top_p =.9, do_sample = True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yk0TKq0qtbw7",
        "outputId": "3fc5f54b-5cb8-463e-a44a-f0a3715c5156"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The dog Joey is jumping over the edge to take the picture as his mother and father look on in horror at his situation. The footage']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_topp(max_new_tokens, top_p, text = text):\n",
        "    text_ids = tokenizer(text, return_tensors = 'pt').input_ids.to(device)\n",
        "    for i in range(max_new_tokens):\n",
        "        torch.manual_seed(42)\n",
        "        output = model(text_ids).logits[0,-1,:]\n",
        "        probs = torch.softmax(output, -1)\n",
        "        probs_sorted_values, probs_sorted_indices = torch.sort(probs, dim = - 1, descending = True)\n",
        "        j = 0\n",
        "        while torch.sum(probs_sorted_values[:j], dim = -1) < top_p: #check for top_p threshold\n",
        "            j += 1\n",
        "        probs_selected = probs_sorted_values[:j] /torch.sum(probs_sorted_values[:j]) # normalize selected probs\n",
        "\n",
        "        m = Categorical(probs = probs_selected[:j]) # sample from allowed words\n",
        "        sampled_id = probs_sorted_indices[m.sample()]\n",
        "        text_ids = torch.cat([text_ids[0],sampled_id.unsqueeze(0)]).unsqueeze(0).to(device)\n",
        "    return text_ids"
      ],
      "metadata": {
        "id": "FWnck5Hs8JnM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.batch_decode(generate_text_topp(20,0.9)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEdBvm0Jjmwu",
        "outputId": "b3728796-51cf-453e-8920-b83d35a970fe"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The dog Joey is jumping over the carpet, always challenging the gardener to calm down.\\n\\n\"He\\'s going to end up']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like the generate functions output a different sentence then our solution. Similar, to top_k, I assume that sampling in the generate function is just different than our approach."
      ],
      "metadata": {
        "id": "c0e6wkCPzNCZ"
      }
    }
  ]
}